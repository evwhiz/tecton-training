{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evwhiz/tecton-training/blob/main/01_End_To_End_Scenario_Snowflake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e32f2678-fcad-436d-8276-1e2fb38958e6",
          "showTitle": false,
          "title": ""
        },
        "id": "UMsXVMEgGn2k"
      },
      "source": [
        "# Lesson: Tecton End-To-End Scenario\n",
        "In lesson you will:\n",
        "* Build an end-to-end feature pipelines using the core Tecton components:\n",
        "  * Data Source\n",
        "  * Entity\n",
        "  * Feature View\n",
        "  * Feature Service\n",
        "* Construct a Training DataSet by querying a Feature Service for historical feature values.\n",
        "* Train a machine learning model.\n",
        "* Make real-time predictions using features retreived from the Tecton online feature store.\n",
        "\n",
        "In this scenario we will work with a dataset of sample purchase transactions from an e-commerce website with the goal of predicting fraud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a680a4cd-b6be-4eeb-b279-515a76fe57ff",
          "showTitle": false,
          "title": ""
        },
        "id": "YaZda8Q4LyhQ"
      },
      "outputs": [],
      "source": [
        "%pip install tecton[rift] scikit-learn snowflake-snowpark-python[pandas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4fa441e9-4b3a-49ee-afbc-f69da73ded3b",
          "showTitle": false,
          "title": ""
        },
        "id": "-0EVYy03Gn2p"
      },
      "source": [
        "## Connect to Tecton\n",
        "To get started, we need to import the Tecton module and login to the Tecton workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9f9ed5f5-9bcb-4ea7-bf6b-8c8d4061c031",
          "showTitle": false,
          "title": ""
        },
        "id": "LL742nTkYDf0"
      },
      "outputs": [],
      "source": [
        "# Import the tecton module\n",
        "import tecton\n",
        "tecton.version.summary()\n",
        "\n",
        "# Automatically validate new tecton objects (recommended).\n",
        "tecton.set_validation_mode(\"auto\")\n",
        "\n",
        "# Login to Tecton, if not already logged-in.\n",
        "tecton.login(\"lab.tecton.ai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6030b994-2ea8-4b8a-b0d3-dbb192642da9",
          "showTitle": false,
          "title": ""
        },
        "id": "YuxohxgZGn2q"
      },
      "source": [
        "## Sample Data\n",
        "The following dataset simulatings credit card transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d586e4ea-b67c-4025-9bb3-8998e2d0163b",
          "showTitle": false,
          "title": ""
        },
        "id": "avBR4tCIWrat"
      },
      "outputs": [],
      "source": [
        "# Set snowflake connection parameters\n",
        "%env SNOWFLAKE_USER=NAB_MFT\n",
        "%env SNOWFLAKE_PASSWORD=G#Rwzc54#z7LE9L\n",
        "%env SNOWFLAKE_ACCOUNT=tectonpartner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1e1f44f1-3ae0-4962-b346-2a3fb02c5e84",
          "showTitle": false,
          "title": ""
        },
        "id": "707vDukZWvf_"
      },
      "outputs": [],
      "source": [
        "# Create a Snowflake connection\n",
        "import os, snowflake\n",
        "connection_parameters = {\n",
        "    'user': os.environ['SNOWFLAKE_USER'],\n",
        "    'password': os.environ['SNOWFLAKE_PASSWORD'],\n",
        "    'account': os.environ['SNOWFLAKE_ACCOUNT'],\n",
        "    'warehouse': 'NAB_WH',\n",
        "    'database': 'NAB',\n",
        "    'schema': 'PUBLIC',\n",
        "}\n",
        "snowflake_conn = snowflake.connector.connect(**connection_parameters)\n",
        "\n",
        "# Configure Tecton to use this Snowflake connection for all interactive queries\n",
        "tecton.snowflake_context.set_connection(snowflake_conn)\n",
        "\n",
        "# A quick helper function to query snowflake from a notebook\n",
        "def query_snowflake(query):\n",
        "    df = snowflake_conn.cursor().execute(query).fetch_pandas_all()\n",
        "    return df\n",
        "\n",
        "# Here's our Snowflake data\n",
        "query_snowflake(\"SELECT * FROM transactions_2 LIMIT 5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4cb5faa7-cef5-41d5-aa7b-cfe593de8cda",
          "showTitle": false,
          "title": ""
        },
        "id": "Ai7vI5KQGn2t"
      },
      "source": [
        "## Key Concept: DataSource\n",
        "A **Tecton DataSource** tells Tecton where to obtain input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "39e6e84c-d63c-4e09-8dd3-90df5baa8783",
          "showTitle": false,
          "title": ""
        },
        "id": "U6f8DmH7SKiz"
      },
      "outputs": [],
      "source": [
        "from tecton import BatchSource, SnowflakeConfig\n",
        "\n",
        "transactions = BatchSource(\n",
        "    name='transactions',\n",
        "    batch_config=SnowflakeConfig(\n",
        "        url=\"https://tectonpartner.snowflakecomputing.com/\",\n",
        "        warehouse=\"NAB_WH\",\n",
        "        database=\"NAB\",\n",
        "        schema=\"PUBLIC\",\n",
        "        table=\"TRANSACTIONS_2\",\n",
        "        timestamp_field=\"TIMESTAMP\"\n",
        "    ),\n",
        "    owner='dbateman@tecton.ai',\n",
        "    tags={'release': 'production'},\n",
        ")\n",
        "\n",
        "# Display sample data\n",
        "display(transactions.get_dataframe().to_pandas().head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "02af4a6c-b17f-4d0e-a558-bb6ba22272a1",
          "showTitle": false,
          "title": ""
        },
        "id": "D3vgPGcjGn2u"
      },
      "source": [
        "## Key Concept: Entity\n",
        "A **Tecton Entity** defines the business-concept for which we are modeling features.  The join keys will be used to aggregate, join, and retrieve features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5d319957-89e5-4725-b605-f67b80a29f2b",
          "showTitle": false,
          "title": ""
        },
        "id": "3Nds_UocTVam"
      },
      "outputs": [],
      "source": [
        "from tecton import Entity, batch_feature_view, Aggregation\n",
        "from tecton.types import Field, String, Timestamp, Float64\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "user = Entity(name=\"user\", join_keys=[\"user_id\"])\n",
        "user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dc455c63-aa43-4659-a81e-f152bd28b27f",
          "showTitle": false,
          "title": ""
        },
        "id": "dqEH358eGn2u"
      },
      "source": [
        "## Key Concept: FeatureView\n",
        "A **Tecton FeatureView** is how ETL pipelines for features are defined in Tecton:\n",
        "* It is the fundamental unit of feature materialization and storage in the Tecton.\n",
        "* SQL or DataFrames transform the raw data and Tecton aggregations to efficiently\n",
        "  and accurately compute metrics across raw events.\n",
        "* The Feature View decorators contain a wide range of attributes for materializing,\n",
        "  cataloging, and monitoring feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0b04faaf-21ba-44cf-8bce-dff85b66442b",
          "showTitle": false,
          "title": ""
        },
        "id": "kYfRaI6JTgww"
      },
      "outputs": [],
      "source": [
        "@batch_feature_view(\n",
        "    description=\"User transaction metrics over 1, 3 and 7 days\",\n",
        "    sources=[transactions],\n",
        "    entities=[user],\n",
        "    mode=\"snowflake_sql\",\n",
        "    schema=[Field(\"user_id\", String), Field(\"timestamp\", Timestamp), Field(\"amt\", Float64)],\n",
        "    aggregation_interval=timedelta(days=1),\n",
        "    aggregations=[\n",
        "        Aggregation(function=\"mean\", column=\"amt\", time_window=timedelta(days=1)),\n",
        "        Aggregation(function=\"mean\", column=\"amt\", time_window=timedelta(days=3)),\n",
        "        Aggregation(function=\"mean\", column=\"amt\", time_window=timedelta(days=7)),\n",
        "        Aggregation(function=\"count\", column=\"amt\", time_window=timedelta(days=1)),\n",
        "        Aggregation(function=\"count\", column=\"amt\", time_window=timedelta(days=3)),\n",
        "        Aggregation(function=\"count\", column=\"amt\", time_window=timedelta(days=7)),\n",
        "    ],\n",
        "    online=True,\n",
        "    offline=True,\n",
        "    feature_start_time=datetime(2021, 1, 1),\n",
        "    batch_schedule=timedelta(days=1),\n",
        "    environment=\"tecton-rift-core-0.9.0\",\n",
        ")\n",
        "def user_transaction_metrics(transactions):\n",
        "    # ATTENTION: Snowflake columns all caps by default and must be quoted to match the lowercase field names above.\n",
        "    return f'''\n",
        "        SELECT USER_ID AS \"user_id\",\n",
        "               TIMESTAMP AS \"timestamp\",\n",
        "               AMT AS \"amt\"\n",
        "        FROM {transactions}\n",
        "    '''\n",
        "\n",
        "# Display Sample Data\n",
        "start = datetime(2021, 1, 1)\n",
        "end = datetime(2099, 1, 1)\n",
        "df = user_transaction_metrics.get_features_in_range(start_time=start, end_time=end).to_pandas()\n",
        "display(df.sort_values([\"user_id\", \"_valid_from\"]).head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ea9f60eb-5bad-4f71-82e6-c78fe80fd4c1",
          "showTitle": false,
          "title": ""
        },
        "id": "Ad2nRtzLGn2v"
      },
      "source": [
        "## Key Concept: Feature Service\n",
        "A **Tecton FeatureService** defines an API endpoint for querying for features from one or more Feature Views."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b751cda8-ed81-43d3-8026-ce749e749f50",
          "showTitle": false,
          "title": ""
        },
        "id": "sp974Qq-ZF1F"
      },
      "outputs": [],
      "source": [
        "from tecton import FeatureService\n",
        "\n",
        "fraud_detection_feature_service = FeatureService(\n",
        "    name=\"fraud_detection_feature_service\", features=[user_transaction_metrics]\n",
        ")\n",
        "fraud_detection_feature_service.get_feature_columns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "77d0ea99-edc0-431d-aeaa-7f34544e8cdb",
          "showTitle": false,
          "title": ""
        },
        "id": "vxbtnIwYGn2v"
      },
      "source": [
        "## Constructing a Training Dataset\n",
        "To perform a batch query of a FeatureService to\n",
        "construct a training dataset, we first must construct a DataFrame specifying\n",
        "the entities and point in time for which we want the historical feature values.\n",
        "We also include any training labels we want to associate with the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9e756717-d6a6-4e15-a571-7e17d9f6e1f7",
          "showTitle": false,
          "title": ""
        },
        "id": "Z3t7YmJwi_10"
      },
      "outputs": [],
      "source": [
        "training_events = query_snowflake('''\n",
        "    SELECT USER_ID AS \"user_id\",\n",
        "           TIMESTAMP AS \"timestamp\",\n",
        "           AMT AS \"amt\",\n",
        "           IS_FRAUD AS \"is_fraud\"\n",
        "    FROM TRANSACTIONS_2\n",
        "    LIMIT 1000\n",
        "''')\n",
        "\n",
        "display(training_events.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2aa1f8bb-7020-4c69-9c90-5e45ac613d20",
          "showTitle": false,
          "title": ""
        },
        "id": "w9sf8aw7i1rg"
      },
      "outputs": [],
      "source": [
        "training_data = fraud_detection_feature_service.get_features_for_events(training_events).to_pandas().fillna(0)\n",
        "display(training_data.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "98392719-ce99-4f8e-9a87-0c81d399fb96",
          "showTitle": false,
          "title": ""
        },
        "id": "jVt8e_dxGn2w"
      },
      "source": [
        "## Use Scikit-Learn to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "667ceb67-0ffb-4d47-9235-dd4bb271fa84",
          "showTitle": false,
          "title": ""
        },
        "id": "aIHkFUGPZ2fQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "df = training_data.drop([\"user_id\", \"timestamp\", \"amt\"], axis=1)\n",
        "\n",
        "X = df.drop(\"is_fraud\", axis=1)\n",
        "y = df[\"is_fraud\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "num_cols = X_train.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
        "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "num_pipe = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
        "\n",
        "cat_pipe = make_pipeline(\n",
        "    SimpleImputer(strategy=\"constant\", fill_value=\"N/A\"), OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        ")\n",
        "\n",
        "full_pipe = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)])\n",
        "\n",
        "model = make_pipeline(full_pipe, LogisticRegression(max_iter=1000, random_state=42))\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_predict = model.predict(X_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_predict, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "959319df-3e59-4259-b267-5c6a5925a24c",
          "showTitle": false,
          "title": ""
        },
        "id": "njogo9EDqs4A"
      },
      "source": [
        "## Deploy to a live Tecton Workspace\n",
        "\n",
        "We'd now like to query the feature service to make predictions.  This involves querying the Online Feature Store.  Before we can query the online feature store we must copy our feature definitions to a feature repository and then run `tecton apply` to deploy it to a live Tecton Workspace and launch the materialization jobs.\n",
        "\n",
        "\n",
        "### Step 1.\n",
        "Copy the code above into a local python project.  (The consolidated code is included below as `features.py`.)\n",
        "\n",
        "\n",
        "### Step 2.\n",
        "Run these shell commands (skipping the # ones):\n",
        "```\n",
        "pip install tecton\n",
        "tecton init\n",
        "tecton login lab.tecton.ai\n",
        "# tecton workspace create \"[your-name]-quickstart\" --live\n",
        "tecton workspace select \"[your-name]-quickstart\"\n",
        "tecton apply\n",
        "tecton service-account create --name \"[your-name]-quickstart\" --description \"Quickstart service account\"\n",
        "tecton access-control assign-role -r consumer -w \"[your-name]-quickstart\" -s \"[service-account-id from last command]\"\n",
        "```\n",
        "\n",
        "### File: `Features.py`\n",
        "```\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import snowflake\n",
        "import tecton\n",
        "from tecton import BatchSource, SnowflakeConfig\n",
        "from tecton import Entity, batch_feature_view, Aggregation, FeatureService\n",
        "from tecton.types import Field, String, Timestamp, Float64\n",
        "\n",
        "\n",
        "connection_parameters = {\n",
        "    'user': 'NAB_MFT',\n",
        "    'password': 'G#Rwzc54#z7LE9L',\n",
        "    'account': 'tectonpartner',\n",
        "    'warehouse': 'NAB_WH',\n",
        "    'database': 'NAB',\n",
        "    'schema': 'PUBLIC',\n",
        "}\n",
        "snowflake_conn = snowflake.connector.connect(**connection_parameters)\n",
        "tecton.snowflake_context.set_connection(snowflake_conn)\n",
        "\n",
        "def query_snowflake(query):\n",
        "    df = snowflake_conn.cursor().execute(query).fetch_pandas_all()\n",
        "    return df\n",
        "\n",
        "transactions = BatchSource(\n",
        "    name='transactions',\n",
        "    batch_config=SnowflakeConfig(\n",
        "        url=\"https://tectonpartner.snowflakecomputing.com/\",\n",
        "        warehouse=\"NAB_WH\",\n",
        "        database=\"NAB\",\n",
        "        schema=\"PUBLIC\",\n",
        "        table=\"TRANSACTIONS_2\",\n",
        "        timestamp_field=\"TIMESTAMP\"\n",
        "    ),\n",
        "    owner='dbateman@tecton.ai',\n",
        "    tags={'release': 'production'},\n",
        ")\n",
        "\n",
        "user = Entity(name=\"user\", join_keys=[\"user_id\"])\n",
        "\n",
        "@batch_feature_view(\n",
        "    description=\"User transaction metrics over 1, 3 and 7 days\",\n",
        "    sources=[transactions],\n",
        "    entities=[user],\n",
        "    mode=\"snowflake_sql\",\n",
        "    aggregation_interval=timedelta(days=1),\n",
        "    aggregations=[\n",
        "        Aggregation(function=\"mean\", column=\"amt\", time_window=timedelta(days=1)),\n",
        "        Aggregation(function=\"mean\", column=\"amt\", time_window=timedelta(days=3)),\n",
        "        Aggregation(function=\"mean\", column=\"amt\", time_window=timedelta(days=7)),\n",
        "        Aggregation(function=\"count\", column=\"amt\", time_window=timedelta(days=1)),\n",
        "        Aggregation(function=\"count\", column=\"amt\", time_window=timedelta(days=3)),\n",
        "        Aggregation(function=\"count\", column=\"amt\", time_window=timedelta(days=7)),\n",
        "    ],\n",
        "    schema=[Field(\"user_id\", String), Field(\"timestamp\", Timestamp), Field(\"amt\", Float64)],\n",
        "    online=True,\n",
        "    offline=True,\n",
        "    feature_start_time=datetime(2021, 1, 1),\n",
        "    batch_schedule=timedelta(days=1),    \n",
        ")\n",
        "def user_transaction_metrics(transactions):\n",
        "    return f'''\n",
        "        SELECT USER_ID AS \"user_id\",\n",
        "               TIMESTAMP AS \"timestamp\",\n",
        "               AMT AS \"amt\"\n",
        "        FROM {transactions}\n",
        "    '''\n",
        "\n",
        "fraud_detection_feature_service = FeatureService(\n",
        "    name=\"fraud_detection_feature_service\", features=[user_transaction_metrics]\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "07d6cc7b-d38c-4f9b-907f-a291f69bd2bd",
          "showTitle": false,
          "title": ""
        },
        "id": "CyDB_rgeGn2w"
      },
      "source": [
        "## Query for features in real-time\n",
        "Query the online feature store via the FeatureService to obtain features to use to make predictions for a user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "33dcc312-e405-4667-9301-6a998fb64760",
          "showTitle": false,
          "title": ""
        },
        "id": "y_CwCKc4fCVH"
      },
      "outputs": [],
      "source": [
        "import requests, json\n",
        "\n",
        "\n",
        "def get_online_feature_data(user_id):\n",
        "    TECTON_API_KEY = \"f136aefbd94e99409bb73a0d76aaa16d\"\n",
        "    WORKSPACE_NAME = \"tecton-quickstart\"\n",
        "    ACCOUNT_NAME = \"lab\"\n",
        "\n",
        "    online_feature_data = requests.request(\n",
        "        method=\"POST\",\n",
        "        url=f\"https://{ACCOUNT_NAME}.tecton.ai/api/v1/feature-service/get-features\",\n",
        "        headers={\n",
        "            \"Authorization\": \"Tecton-key \" + TECTON_API_KEY,\n",
        "        },\n",
        "        json={\n",
        "            \"params\": {\n",
        "                \"feature_service_name\": \"fraud_detection_feature_service\",\n",
        "                \"join_key_map\": {\"user_id\": user_id},\n",
        "                \"metadata_options\": {\"include_names\": True},\n",
        "                \"workspace_name\": WORKSPACE_NAME,\n",
        "            }\n",
        "        },\n",
        "    )\n",
        "\n",
        "    return online_feature_data.json()\n",
        "\n",
        "\n",
        "user_id = \"user_502567604689\"\n",
        "feature_data = get_online_feature_data(user_id)\n",
        "if \"result\" not in feature_data:\n",
        "    print(\"Feature data is not materialized\")\n",
        "else:\n",
        "    print(feature_data[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "553444a2-b322-4527-9257-05cc2ccf6e5e",
          "showTitle": false,
          "title": ""
        },
        "id": "JMyohWIFGn2w"
      },
      "source": [
        "## Perform real-time inferencing\n",
        "Make a prediction by inputing the features into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a5c4ecad-6cc3-4b67-8fd0-badb71bf95b6",
          "showTitle": false,
          "title": ""
        },
        "id": "jDJPKomZfgeJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_prediction_from_model(feature_data):\n",
        "    columns = [f[\"name\"].replace(\".\", \"__\") for f in feature_data[\"metadata\"][\"features\"]]\n",
        "    data = [feature_data[\"result\"][\"features\"]]\n",
        "    features = pd.DataFrame(data, columns=columns)[X.columns]\n",
        "    return model.predict_proba(features)[0][1]\n",
        "\n",
        "def evaluate_transaction(user_id):\n",
        "    online_feature_data = get_online_feature_data(user_id)\n",
        "    is_predicted_fraud = get_prediction_from_model(online_feature_data)\n",
        "    if is_predicted_fraud <= 0.5:\n",
        "        return \"Transaction accepted.\"\n",
        "    else:\n",
        "        return \"Transaction denied.\"\n",
        "\n",
        "evaluate_transaction(\"user_502567604689\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "01 - End-To-End Scenario - Snowflake",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}